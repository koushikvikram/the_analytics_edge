{
    "collab_server" : "",
    "contents" : "# read in the data set\nwine = read.csv(\"wine.csv\")\n\nstr(wine)\n# Year is an identifier\n# we take Price to be our dependent variable\n# WinterRain, AGST, HarvestRain, Age and FrancePop are our independent variables\n\nsummary(wine)\n\n########## single variable linear regression using AGST to predict Price\n# syntax for linear regression: lm(dependentVariable ~ independentVariable(s), data=dataSet)\nmodel1  = lm(Price ~ AGST, data=wine) # lm stands for linear model\n\nsummary(model1) # Call, \n# Residuals - the error values for each point \n# Coefficients - the values in (Estimate) are beta0, beta1, etc \n# Multiple R-squared - value of R squared due to all independent variables (never decreases)\n# Adjusted R-squared -  adjusts the R squared value to account for the number of independent variables used relative to the \n# number of data points (will decrease if you add an independent variable that doesn't help the model)\n\n# Multiple R-squared = 0.435, Adjusted R-squared = 0.4105 \n# b0 = -3.4178, b1 = 0.6351\n\nmodel1$residuals # errors\n\n# SSE for model1\nSSE = sum(model1$residuals^2)\nSSE # 5.734875\n\n########## multiple linear regression dep: Price, indep: AGST + HarvestRain\nmodel2 = lm(Price ~ AGST + HarvestRain, data=wine)\n\nsummary(model2)\n# Multiple R-squared = 0.7074, Adjusted R-squared = 0.6808\n# b0 = -2.20265, b1 = 0.60262, b2 = -0.00457\n\nSSE = sum(model2$residuals^2)\nSSE # 2.970373\n\n########## multiple linear regression dep: Price, indep: AGST + HarvestRain + WinterRain + Age + FrancePop\nmodel3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)\n\nsummary(model3)\n# b0 = -0.4504, b1 = 0.6012, b2 = -0.003958, b3 = 0.001043, b4 = 0.0005847, b5 = -0.00004953\n# Multiple R-squared = 0.8294, Adjusted R-squared = 0.7845\n\nSSE = sum(model3$residuals^2)\nSSE # 1.732113\n\n### Understanding the model\n# We see that AGST and HarvestRain have '***' next to them. So, they are the most significant. \n# WinterRain has a '.' next to it. So, it is significant too.\n# Age and FrancePop have no Significance codes next to them. So, they are insignificant and can be removed.\n\n# Let's start by removing FrancePop, which we intuitively don't expect to be predictive of wine price anyway.\nmodel4 = lm(Price ~ AGST + HarvestRain + WinterRain + Age, data=wine)\nsummary(model4)\n# Multiple R-squared = 0.8286, Adjusted R-squared = 0.7943\n# Adjusted R-squared is higher than that of model3's \n\n# now, we see that Age has '**' next to it. So, it is pretty significant now.\n# earlier, age was not significant, but after FrancePop was removed, it has become more significant.\n# This is because of multicollinearity. Age and FrancePopulation are highly correlated.\n\n########## Correlation and Multicollinearity\n# Correlation - a measure of the linear relationship between variables\n# +1 = perfect positive linear relationship\n# 0 = no linear relationship\n# -1 = perfect negative linear relationship\n\n# correlation between WinterRain and Price\ncor(wine$WinterRain, wine$Price) # 0.1366505 - low correlation\n\n# correlation between Age and FrancePop\ncor(wine$Age, wine$FrancePop) # -0.9944851 - very high correlation\n\n# compute the correlation between all pairs of variables in our data set\ncor(wine)\n\n# Multicollinearity - a situation when two independent variables are highly correlated\n# We've confirmed that Age and FrancePop are highly correlated.\n# So, we have multicollinearity problems in our model that uses all the independent variables.\n# High correlation between an independent variable and dependent variable is a good thing\n\n# Due to the problem of multicollinearity, you always want to remove the independent variables one at a time\n\n# Let's see what would have happened if we had removed both Age and FrancePop at the same time, since they were both insignificant\nmodel5 = lm(Price ~ AGST + HarvestRain + WinterRain ,data=wine)\nsummary(model5) # now, we see that AGST and HarvesRain are very significant, while WinterRain is slightly significant\n# Multiple R-squared = 0.7537, Adjusted R-squared = 0.7185\n\n# model4 has higher R-squared values\n# So, if we had removed Age and FrancePop at the same time, we would have missed a significant variable, and the R-squared of our \n# final model would have been lower\n\n# Why didn't we keep FrancePop instead of Age?\n# We expect Age to be significant. Older wines are typically more expensive. So, Age makes more intuitive sense in our model.\n\n# Multicollinearity reminds us that coefficients are only interpretable in the presence of other variables being used.\n# cut-off value for what makes a correlation too high - no definitve answer, but generally > 0.7 or < -0.7 is cause for concern.\n\n########## multiple linear regression dep: Price, indep: HarvestRain + WinterRain\nmodel6 = lm(Price ~ HarvestRain + WinterRain, data=wine)\n\nsummary(model6)\n# b0 = 7.865, b1 = -0.004971, b2 = -0.00009848\n# Multiple R-squared = 0.3177, Adjusted R-squared = 0.2557\n\nSSE = sum(model6$residuals^2)\nSSE # 6.925756\n\n\n########## Making predictions\n# Let's see how well our model perfoms on some test data in R.\n# We have two data points that we did not use to build our model in the file \"wine_test.csv\"\nwineTest = read.csv(\"wine_test.csv\")\nstr(wineTest)\n\n# let's make predictions for these two test points\npredictTest = predict(model4, newdata = wineTest)\npredictTest # predicted price for 1st data point = 6.768925 and for 2nd data point = 6.684910\n# actual price for 1st data point = 6.95 and 2nd data point = 6.5\n# so, it looks like our predictions are pretty good.\n\n# let's quantify this by computing R-squared value for our test set\nSSE = sum((wineTest$Price - predictTest)^2)\nSST = sum((wineTest$Price - mean(wine$Price))^2)\n1 - SSE/SST # 0.7944278\n\n# keep in mind that our test set is very small. We should increase the size of our test set to be more confident about the \n# out-of-sample accuracy of our model.\n\n\n\n",
    "created" : 1498518925238.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1234009408",
    "id" : "A622ADD2",
    "lastKnownWriteTime" : 1500254352,
    "last_content_update" : 1500254352841,
    "path" : "~/Desktop/courses/edx/the_analytics_edge/mit_the_analytics_edge/unit_2_linear_regression/linear_regression.R",
    "project_path" : "linear_regression.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}